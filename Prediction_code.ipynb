{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9077d94f-4562-480a-8403-e119753fef91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546a053e-a387-45ca-b647-de1a73c75d30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3defc8-993c-4bcb-85a1-4d6014ed6e96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73b496a2-3626-46c8-a98c-57dc7db0c23c",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c207e1fe-c5bc-4942-a588-ea1e1020c6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rames\\anaconda3\\envs\\Biometrics\\lib\\site-packages\\transformers\\models\\vit\\feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n",
      "Using cache found in C:\\Users\\rames/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n",
      "C:\\Users\\rames\\anaconda3\\envs\\Biometrics\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\rames\\anaconda3\\envs\\Biometrics\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting from Face...\n",
      "Face Prediction: Aparna\n",
      "\n",
      "Predicting from Iris...\n",
      "Iris Prediction: Aparna\n",
      "\n",
      "Predicting from Fingerprint...\n",
      "Fingerprint Prediction: Aparna\n",
      "\n",
      ":white_check_mark: Person is AUTHORIZED\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pickle\n",
    "import joblib\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from transformers import ViTModel, ViTFeatureExtractor\n",
    "import pywt\n",
    "# Step 2: Load Saved Models and Feature Files\n",
    "# Face\n",
    "face_model = joblib.load(\"random_forest_face_model.pkl\")\n",
    "with open(\"face_features_and_labels.pkl\", 'rb') as f:\n",
    "    face_data = pickle.load(f)\n",
    "face_encoder = face_data[\"label_encoder\"]\n",
    "# Iris\n",
    "iris_model = joblib.load(\"rf_iris_model_wtn_vit.pkl\")\n",
    "with open(\"iris_features_wtn_vit.pkl\", 'rb') as f:\n",
    "    iris_data = pickle.load(f)\n",
    "iris_encoder = iris_data[\"label_encoder\"]\n",
    "# Fingerprint\n",
    "finger_model = joblib.load(\"random_forest_fingerprint_model_cnn_wtn.pkl\")\n",
    "with open(\"fingerprint_features_cnn_wtn.pkl\", 'rb') as f:\n",
    "    finger_data = pickle.load(f)\n",
    "finger_encoder = finger_data[\"label_encoder\"]\n",
    "# Step 3: Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Step 4: Load CNN model used for fingerprint feature extraction\n",
    "import torch.nn as nn\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.fc1 = nn.Linear(32 * 56 * 56, 256)\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 32 * 56 * 56)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return x\n",
    "finger_cnn_model = SimpleCNN().to(device)\n",
    "finger_cnn_model.eval()\n",
    "# Step 5: ViT Setup\n",
    "vit_model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k').to(device).eval()\n",
    "vit_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "# Step 6: Feature Extraction Functions\n",
    "# --- Face ---\n",
    "cnn_model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)\n",
    "cnn_model = nn.Sequential(*list(cnn_model.children())[:-1]).to(device).eval()\n",
    "cnn_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "def extract_face_features(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        return None\n",
    "    image_rgb = cv2.cvtColor(cv2.resize(image, (224, 224)), cv2.COLOR_BGR2RGB)\n",
    "    # CNN features\n",
    "    img_tensor = cnn_transform(image_rgb).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        cnn_feat = cnn_model(img_tensor).squeeze().cpu().numpy()\n",
    "    # ViT features\n",
    "    inputs = vit_extractor(images=image_rgb, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        vit_feat = vit_model(**inputs).pooler_output.squeeze().cpu().numpy()\n",
    "    combined = np.concatenate((cnn_feat, vit_feat))  # 2048 + 768\n",
    "    return combined\n",
    "# --- Iris ---\n",
    "def wavelet_transform(image):\n",
    "    coeffs = pywt.wavedec2(image, 'haar', level=2)\n",
    "    flattened = []\n",
    "    for coeff in coeffs:\n",
    "        if isinstance(coeff, tuple):\n",
    "            for arr in coeff:\n",
    "                flattened.extend(arr.flatten())\n",
    "        else:\n",
    "            flattened.extend(coeff.flatten())\n",
    "    return np.array(flattened[:4096])\n",
    "def extract_iris_features(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        return None\n",
    "    image_resized = cv2.resize(image, (224, 224))\n",
    "    gray = cv2.cvtColor(image_resized, cv2.COLOR_BGR2GRAY)\n",
    "    # WTN features\n",
    "    wavelet_feat = wavelet_transform(gray)\n",
    "    # ViT features\n",
    "    inputs = vit_extractor(images=image_resized, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        vit_feat = vit_model(**inputs).pooler_output.squeeze().cpu().numpy()\n",
    "    combined = np.concatenate((wavelet_feat, vit_feat))  # 4096 + 768\n",
    "    return combined\n",
    "# --- Fingerprint ---\n",
    "transform_finger = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "def extract_fingerprint_features(image_path):\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if image is None:\n",
    "        return None\n",
    "    image_resized = cv2.resize(image, (224, 224))\n",
    "    # WTN features\n",
    "    wtn_feat = wavelet_transform(image_resized)[:2048]  # :white_check_mark: Force only 2048 features\n",
    "    # CNN features\n",
    "    input_tensor = transform_finger(image_resized).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        cnn_feat = finger_cnn_model(input_tensor).cpu().numpy().flatten()\n",
    "    cnn_feat = cnn_feat[:256]  # :white_check_mark: Force only 256 features\n",
    "    # Combine both features\n",
    "    combined = np.concatenate((wtn_feat, cnn_feat))  # 2048 + 256 = 2304\n",
    "    return combined\n",
    "# Step 7: Predict Function\n",
    "def predict_user(face_path, iris_path, fingerprint_path):\n",
    "    print(\"\\nPredicting from Face...\")\n",
    "    face_features = extract_face_features(face_path)\n",
    "    face_pred = face_model.predict([face_features])[0]\n",
    "    face_label = face_encoder.inverse_transform([face_pred])[0]\n",
    "    print(f\"Face Prediction: {face_label}\")\n",
    "    print(\"\\nPredicting from Iris...\")\n",
    "    iris_features = extract_iris_features(iris_path)\n",
    "    iris_pred = iris_model.predict([iris_features])[0]\n",
    "    iris_label = iris_encoder.inverse_transform([iris_pred])[0]\n",
    "    print(f\"Iris Prediction: {iris_label}\")\n",
    "    print(\"\\nPredicting from Fingerprint...\")\n",
    "    fingerprint_features = extract_fingerprint_features(fingerprint_path)\n",
    "    finger_pred = finger_model.predict([fingerprint_features])[0]\n",
    "    finger_label = finger_encoder.inverse_transform([finger_pred])[0]\n",
    "    print(f\"Fingerprint Prediction: {finger_label}\")\n",
    "    # Final Decision\n",
    "    if face_label == iris_label == finger_label:\n",
    "        print(\"\\n:white_check_mark: Person is AUTHORIZED\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"\\n:no_entry_sign: Person is NOT AUTHORIZED\")\n",
    "        return False\n",
    "# Step 8: Run Prediction\n",
    "if __name__ == \"__main__\":\n",
    "    face_img = r\"C:\\Users\\rames\\OneDrive\\Desktop\\Biometrics\\data\\face\\Test\\Aparna\\Aparna_1.jpg\"\n",
    "    iris_img = r\"C:\\Users\\rames\\OneDrive\\Desktop\\Biometrics\\data\\iris\\Test\\Aparna\\Aparna_1.jpg\"\n",
    "    finger_img = r\"C:\\Users\\rames\\OneDrive\\Desktop\\Biometrics\\data\\fingerprint\\Test\\Aparna\\Aparna_1.jpg\"\n",
    "    predict_user(face_img, iris_img, finger_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a5063a-4bbb-4f9b-bb8d-995084363672",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3166788f-4b62-41eb-ad98-f003adf5f3a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb4bce2-4945-4455-9f19-5423ab89f620",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66172cb-757e-4ad8-8c93-4b14dee83dea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3548f2f7-f28d-4d1f-9b95-2785ca771d9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67570ab-21f5-4142-8319-9afd3ca18eb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a2545e-06f4-48a1-9fc7-d6fd3dad2e67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f96462-7175-415f-b191-76a8e2623d41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
